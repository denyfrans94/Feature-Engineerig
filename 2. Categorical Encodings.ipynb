{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Now that you've built a baseline model, you are ready to improve it with some clever ways to convert categorical variables into numerical features. These encodings will be learned from the data itself. The most basic encodings are one-hot encoding (covered in the Intermediate Machine Learning course) and label encoding which you saw in the first tutorial.\n",
    "\n",
    "Here, you'll learn count encoding, target encoding (and variations), and singular value decomposition.\n",
    "\n",
    "Here is the code to rebuild the baseline model from the first tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "                                                                                                                                                                                                                                                                                                    \n",
    "ks = pd.read_csv('Dataset/kickstarter-projects/ks-projects-201801.csv', parse_dates=['deadline', 'launched'])\n",
    "ks.head()\n",
    "                                                                                                                                                                                                                                                                                                                                                         \n",
    "# Drop live projects\n",
    "ks = ks.query('state != \"live\"')\n",
    "\n",
    "# Add outcome column, \"successful\" == 1, others are 0\n",
    "ks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))\n",
    "\n",
    "# Timestamp features\n",
    "ks = ks.assign(hour=ks.launched.dt.hour,\n",
    "               day=ks.launched.dt.day,\n",
    "               month=ks.launched.dt.month,\n",
    "               year=ks.launched.dt.year)\n",
    "\n",
    "# Label encoding\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "encoder = LabelEncoder()\n",
    "encoded = ks[cat_features].apply(encoder.fit_transform)\n",
    "\n",
    "data_cols = ['goal', 'hour', 'day', 'month', 'year', 'outcome']\n",
    "\n",
    "baseline_data = ks[data_cols].join(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "failed        197719\n",
       "successful    133956\n",
       "canceled       38779\n",
       "undefined       3562\n",
       "suspended       1846\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks.state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>goal</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>outcome</th>\n",
       "      <th>category</th>\n",
       "      <th>currency</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19500.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      goal  hour  day  month  year  outcome  category  currency  country\n",
       "0   1000.0    12   11      8  2015        0       108         5        9\n",
       "1  30000.0     4    2      9  2017        0        93        13       22\n",
       "2  45000.0     0   12      1  2013        0        93        13       22\n",
       "3   5000.0     3   17      3  2012        0        90        13       22\n",
       "4  19500.0     8    4      7  2015        0        55        13       22"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining  functions that will help us test our encodings\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "    valid_fraction = 0.1\n",
    "    valid_size = int(len(dataframe) * valid_fraction)\n",
    "    \n",
    "    train = dataframe[:-valid_size * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_size * 2:-valid_size]\n",
    "    test = dataframe[-valid_size:]\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "\n",
    "def train_model(train, valid):\n",
    "    feature_cols = train.columns.drop('outcome')\n",
    "    \n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n",
    "\n",
    "    param = {'num_leaves':64, 'objective':'binary', \n",
    "             'metric':'auc', 'seed':7}\n",
    "    print('Training model!')\n",
    "    bst = lgb.train(param, dtrain, num_boost_round=1000, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=10, verbose_eval=False)\n",
    "    \n",
    "    \n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['outcome'], valid_pred)\n",
    "    print(f'Validation AUC Score: {valid_score:.4f}')\n",
    "    return bst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC Score: 0.7467\n"
     ]
    }
   ],
   "source": [
    "# Training a model on the baseline data\n",
    "train, valid, _ = get_data_splits(baseline_data)\n",
    "bst = train_model(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Encoding\n",
    "Count encoding replaces each categorical value with the number of times it appears in the dataset. For example, if the value \"GB\" occured 10 times in the country feature, then each \"GB\" would be replaced with the number 10.\n",
    "\n",
    "We'll use the categorical-encodings package to get this encoding. The encoder itself is available as CountEncoder. This encoder and the others in categorical-encodings work like scikit-learn transformers with .fit and .transform methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in c:\\users\\deny frans\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in c:\\users\\deny frans\\anaconda3\\lib\\site-packages (from category_encoders) (0.11.0)\n",
      "Requirement already satisfied: pandas>=0.21.1 in c:\\users\\deny frans\\anaconda3\\lib\\site-packages (from category_encoders) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\deny frans\\anaconda3\\lib\\site-packages (from category_encoders) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy>=0.19.0 in c:\\users\\deny frans\\anaconda3\\lib\\site-packages (from category_encoders) (1.4.1)\n",
      "Requirement already satisfied: patsy>=0.4.1 in c:\\users\\deny frans\\anaconda3\\lib\\site-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\deny frans\\anaconda3\\lib\\site-packages (from category_encoders) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\deny frans\\anaconda3\\lib\\site-packages (from pandas>=0.21.1->category_encoders) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\deny frans\\anaconda3\\lib\\site-packages (from pandas>=0.21.1->category_encoders) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\deny frans\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (0.14.1)\n",
      "Requirement already satisfied: six in c:\\users\\deny frans\\anaconda3\\lib\\site-packages (from patsy>=0.4.1->category_encoders) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deny Frans\\anaconda3\\lib\\site-packages\\category_encoders\\count.py:255: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  X.loc[:, self.cols] = X.fillna(value=pd.np.nan)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC Score: 0.7486\n"
     ]
    }
   ],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "count_enc = ce.CountEncoder()\n",
    "count_encoded = count_enc.fit_transform(ks[cat_features])\n",
    "\n",
    "data = baseline_data.join(count_encoded.add_suffix('count'))\n",
    "\n",
    "# Training a model on the baseline data\n",
    "train, valid, test = get_data_splits(data)\n",
    "bst = train_model(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the count encoding features increase the validation score from 0.7467 to 0.7486, only a slight improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding\n",
    "Target encoding replaces a categorical value with the average value of the target for that value of the feature. For example, given the country value \"CA\", you'd calculate the average outcome for all the rows with country == 'CA', around 0.28. This is often blended with the target probability over the entire dataset to reduce the variance of values with few occurences.\n",
    "\n",
    "This technique uses the targets to create new features. So including the validation or test data in the target encodings would be a form of target leakage. Instead, you should learn the target encodings from the training dataset only and apply it to the other datasets.\n",
    "\n",
    "The category_encoders package provides TargetEncoder for target encoding. The implementation is similar to CountEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC Score: 0.7491\n"
     ]
    }
   ],
   "source": [
    "cat_features = ['category', 'currency', 'country']\n",
    "\n",
    "# Create the encoder itself\n",
    "target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "\n",
    "train, valid, _ = get_data_splits(data)\n",
    "\n",
    "# Fit the encoder using the categorical features and target\n",
    "target_enc.fit(train[cat_features], train['outcome'])\n",
    "\n",
    "# Transform the features, rename the columns with _target suffix, and join to dataframe\n",
    "train = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n",
    "valid = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))\n",
    "\n",
    "train.head()\n",
    "bst = train_model(train,valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation score is higher again, from 0.7467 to 0.7491."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CatBoost Encoding\n",
    "Finally, we'll look at CatBoost encoding. This is similar to target encoding in that it's based on the target probablity for a given value. However with CatBoost, for each row, the target probability is calculated only from the rows before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC Score: 0.7492\n"
     ]
    }
   ],
   "source": [
    "cat_features = ['category', 'currency', 'country']\n",
    "target_enc = ce.CatBoostEncoder(cols=cat_features)\n",
    "\n",
    "train, valid, _ = get_data_splits(data)\n",
    "target_enc.fit(train[cat_features], train['outcome'])\n",
    "\n",
    "train = train.join(target_enc.transform(train[cat_features]).add_suffix('_cb'))\n",
    "valid = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_cb'))\n",
    "\n",
    "bst = train_model(train, valid)\n",
    "\n",
    "# cat_features = ['category', 'currency', 'country']\n",
    "# target_enc = ce.CatBoostEncoder(cols=cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does slightly better than target encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Categorical Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this exercise you'll apply more advanced encodings to encode the categorical variables ito improve your classifier model. The encodings you will implement are:\n",
    "\n",
    "- Count Encoding\n",
    "- Target Encoding\n",
    "- Leave-one-out Encoding\n",
    "- CatBoost Encoding\n",
    "- Feature embedding with SVD \n",
    "\n",
    "You'll refit the classifier after each encoding to check its performance on hold-out data. First, run the next cell to repeat the work you did in the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\deny frans\\anaconda3\\lib\\site-packages (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\deny frans\\anaconda3\\lib\\site-packages (from pyarrow) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27226</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>120</td>\n",
       "      <td>2017-11-06 15:13:23</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110007</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-11-06 15:41:07</td>\n",
       "      <td>2017-11-07 08:17:19</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1047</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>157</td>\n",
       "      <td>2017-11-06 15:42:32</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76270</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>120</td>\n",
       "      <td>2017-11-06 15:56:17</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>56</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57862</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>120</td>\n",
       "      <td>2017-11-06 15:57:01</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel          click_time      attributed_time  \\\n",
       "0   27226    3       1  13      120 2017-11-06 15:13:23                 None   \n",
       "1  110007   35       1  13       10 2017-11-06 15:41:07  2017-11-07 08:17:19   \n",
       "2    1047    6       1  13      157 2017-11-06 15:42:32                 None   \n",
       "3   76270    3       1  13      120 2017-11-06 15:56:17                 None   \n",
       "4   57862    3       1  13      120 2017-11-06 15:57:01                 None   \n",
       "\n",
       "   is_attributed  day  hour  minute  second  \n",
       "0              0    6    15      13      23  \n",
       "1              1    6    15      41       7  \n",
       "2              0    6    15      42      32  \n",
       "3              0    6    15      56      17  \n",
       "4              0    6    15      57       1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks = pd.read_parquet('Dataset/baseline_data.pqt')\n",
    "clicks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'll define a couple functions to help test the new encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "    \"\"\" Splits a dataframe into train, validation, and test sets. First, orders by \n",
    "        the column 'click_time'. Set the size of the validation and test sets with\n",
    "        the valid_fraction keyword argument.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataframe = dataframe.sort_values('click_time')\n",
    "    valid_rows = int(len(dataframe) * valid_fraction)\n",
    "    train = dataframe[:-valid_rows * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_rows * 2:-valid_rows]\n",
    "    test = dataframe[-valid_rows:]\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "def train_model(train, valid, test=None, feature_cols=None):\n",
    "    if feature_cols is None:\n",
    "        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n",
    "                                           'is_attributed'])\n",
    "        \n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
    "    \n",
    "    param = {'num_leaves':64, 'objective':'binary', 'metric':'auc', 'seed':7}\n",
    "    num_round = 1000\n",
    "    print('Training Model')\n",
    "    \n",
    "    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=20, verbose_eval=False)\n",
    "    \n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n",
    "    print(f\"Validation AUC score: {valid_score}\")\n",
    "    \n",
    "    if test is not None: \n",
    "        test_pred = bst.predict(test[feature_cols])\n",
    "        test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n",
    "        return bst, valid_score, test_score\n",
    "    else:\n",
    "        return bst, valid_score    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to get a baseline score. If your encodings do better than this, you can keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model\n",
      "Training Model\n",
      "Validation AUC score: 0.9622743228943659\n"
     ]
    }
   ],
   "source": [
    "print('Baseline model')\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "_ = train_model(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Categorical encodings and leakage\n",
    "\n",
    "These encodings are all based on statistics calculated from the dataset like counts and means. Considering this, what data should you be using to calculate the encodings?\n",
    "\n",
    "Run the following line after you've decided your answer.\n",
    "\n",
    "Solution: You should calculate the encodings from the training set only. If you include data from the validation and test sets into the encodings, you'll overestimate the model's performance. You should in general be vigilant to avoid leakage, that is, including any information from the validation and test sets into the model. For a review on this topic, see our lesson on data leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Count encodings\n",
    "\n",
    "Here, encode the categorical features `['ip', 'app', 'device', 'os', 'channel']` using the count of each value in the data set. Using `CountEncoder` from the `category_encoders` library, fit the encoding using the categorical feature columns defined in `cat_features`. Then apply the encodings to the train and validation sets, adding them as new columns with names suffixed `\"_count\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deny Frans\\anaconda3\\lib\\site-packages\\category_encoders\\count.py:255: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
      "  X.loc[:, self.cols] = X.fillna(value=pd.np.nan)\n"
     ]
    }
   ],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the count encoder\n",
    "count_enc = ce.CountEncoder(cols=cat_features)\n",
    "\n",
    "# Learn encoding from the training set\n",
    "count_enc.fit(train[cat_features])\n",
    "\n",
    "# Apply encoding to the train and validation sets as new columns\n",
    "# Make sure to add `_count` as a suffix to the new columns\n",
    "train_encoded = train.join(count_enc.transform(train[cat_features]).add_suffix('_count'))\n",
    "valid_encoded = valid.join(count_enc.transform(valid[cat_features]).add_suffix('_count'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "Validation AUC score: 0.9653051135205329\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the encoded datasets\n",
    "# This can take around 30 seconds to complete\n",
    "_ = train_model(train_encoded, valid_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count encoding improved our model's score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Why is count encoding effective?\n",
    "At first glance, it could be surprising that Count Encoding helps make accurate models. \n",
    "Why do you think is count encoding is a good idea, or how does it improve the model score?\n",
    "\n",
    "Run the following line after you've decided your answer.\n",
    "\n",
    "Solution: Rare values tend to have similar counts (with values like 1 or 2), so you can classify rare values together at prediction time. Common values with large counts are unlikely to have the same exact count as other values. So, the common/important values get their own grouping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Target encoding\n",
    "\n",
    "Here you'll try some supervised encodings that use the labels (the targets) to transform categorical features. The first one is target encoding. Create the target encoder from the `category_encoders` library. Then, learn the encodings from the training dataset, apply the encodings to all the datasets and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the target encoder. You can find this easily by using tab completion.\n",
    "# Start typing ce. the press Tab to bring up a list of classes and functions.\n",
    "target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "\n",
    "# Learn encoding from the training set. Use the 'is_attributed' column as the target.\n",
    "target_enc.fit(train[cat_features], train['is_attributed'])\n",
    "\n",
    "# Apply encoding to the train and validation sets as new columns\n",
    "# Make sure to add `_target` as a suffix to the new columns\n",
    "train_encoded = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n",
    "valid_encoded = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "Validation AUC score: 0.9540530347873288\n"
     ]
    }
   ],
   "source": [
    "_ = train_model(train_encoded, valid_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Try removing IP encoding\n",
    "\n",
    "Try leaving `ip` out of the encoded features and retrain the model with target encoding again. You should find that the score increases and is above the baseline score! Why do you think the score is below baseline when we encode the IP address but above baseline when we don't?\n",
    "\n",
    "Run the following line after you've decided your answer.\n",
    "\n",
    "Solution: Target encoding attempts to measure the population mean of the target for each level in a categorical feature. This means when there is less data per level, the estimated mean will be further away from the \"true\" mean, there will be more variance. There is little data per IP address so it's likely that the estimates are much noisier than for the other features. The model will rely heavily on this feature since it is extremely predictive. This causes it to make fewer splits on other features, and those features are fit on just the errors left over accounting for IP address. So, the model will perform very poorly when seeing new IP addresses that weren't in the training data (which is likely most new data). Going forward, we'll leave out the IP feature when trying different encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mencoba tanpa kolom IP\n",
    "cat_features = ['app', 'device', 'os', 'channel']\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the target encoder. You can find this easily by using tab completion.\n",
    "# Start typing ce. the press Tab to bring up a list of classes and functions.\n",
    "target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "\n",
    "# Learn encoding from the training set. Use the 'is_attributed' column as the target.\n",
    "target_enc.fit(train[cat_features], train['is_attributed'])\n",
    "\n",
    "# Apply encoding to the train and validation sets as new columns\n",
    "# Make sure to add `_target` as a suffix to the new columns\n",
    "train_encoded = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n",
    "valid_encoded = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "Validation AUC score: 0.9627457957514338\n"
     ]
    }
   ],
   "source": [
    "_ = train_model(train_encoded, valid_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) CatBoost Encoding\n",
    "\n",
    "The CatBoost encoder is supposed to working well with the LightGBM model. Encode the categorical features with `CatBoostEncoder` and train the model on the encoded data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove IP from the encoded features\n",
    "cat_features = ['app', 'device', 'os', 'channel']\n",
    "\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the CatBoost encoder\n",
    "cb_enc = ce.CatBoostEncoder(cols=cat_features)\n",
    "# Learn encoding from the training set\n",
    "cb_enc.fit(train[cat_features], train['is_attributed'])\n",
    "\n",
    "# Apply encoding to the train and validation sets as new columns\n",
    "# Make sure to add `_cb` as a suffix to the new columns\n",
    "train_encoded = train.join(cb_enc.transform(train[cat_features]).add_suffix('_cb'))\n",
    "valid_encoded = valid.join(cb_enc.transform(train[cat_features]).add_suffix('_cb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n",
      "Validation AUC score: 0.9622743228943659\n"
     ]
    }
   ],
   "source": [
    "_ = train_model(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CatBoost encodings work the best, so we'll keep those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = cb_enc.transform(clicks[cat_features])\n",
    "for col in encoded:\n",
    "    clicks.insert(len(clicks.columns), col + '_cb', encoded[col])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
